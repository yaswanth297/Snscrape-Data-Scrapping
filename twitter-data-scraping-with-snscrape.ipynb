{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "905eb33e",
   "metadata": {
    "papermill": {
     "duration": 0.006083,
     "end_time": "2023-05-27T12:07:25.095087",
     "exception": false,
     "start_time": "2023-05-27T12:07:25.089004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SNSCRAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ead7ca74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:07:25.107519Z",
     "iopub.status.busy": "2023-05-27T12:07:25.107149Z",
     "iopub.status.idle": "2023-05-27T12:07:25.112435Z",
     "shell.execute_reply": "2023-05-27T12:07:25.111277Z"
    },
    "papermill": {
     "duration": 0.016822,
     "end_time": "2023-05-27T12:07:25.117165",
     "exception": false,
     "start_time": "2023-05-27T12:07:25.100343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip3 install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cad8529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:07:25.129173Z",
     "iopub.status.busy": "2023-05-27T12:07:25.128766Z",
     "iopub.status.idle": "2023-05-27T12:07:26.214442Z",
     "shell.execute_reply": "2023-05-27T12:07:26.213112Z"
    },
    "papermill": {
     "duration": 1.094988,
     "end_time": "2023-05-27T12:07:26.217225",
     "exception": false,
     "start_time": "2023-05-27T12:07:25.122237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.10\r\n"
     ]
    }
   ],
   "source": [
    "#Supported only by versions higher than 3.8\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc066dfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:07:26.229547Z",
     "iopub.status.busy": "2023-05-27T12:07:26.229159Z",
     "iopub.status.idle": "2023-05-27T12:08:09.338388Z",
     "shell.execute_reply": "2023-05-27T12:08:09.337091Z"
    },
    "papermill": {
     "duration": 43.11896,
     "end_time": "2023-05-27T12:08:09.341365",
     "exception": false,
     "start_time": "2023-05-27T12:07:26.222405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\r\n",
      "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to /tmp/pip-req-build-l00n7_t2\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/JustAnotherArchivist/snscrape.git /tmp/pip-req-build-l00n7_t2\r\n",
      "  Resolved https://github.com/JustAnotherArchivist/snscrape.git to commit 786815dd05681e2421cd03aa9acf5ab5c773bce9\r\n",
      "  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from snscrape==0.6.2.20230321.dev13+g786815d) (2.28.2)\r\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from snscrape==0.6.2.20230321.dev13+g786815d) (4.9.2)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from snscrape==0.6.2.20230321.dev13+g786815d) (4.12.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from snscrape==0.6.2.20230321.dev13+g786815d) (3.12.0)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->snscrape==0.6.2.20230321.dev13+g786815d) (2.3.2.post1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->snscrape==0.6.2.20230321.dev13+g786815d) (2.1.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->snscrape==0.6.2.20230321.dev13+g786815d) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->snscrape==0.6.2.20230321.dev13+g786815d) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->snscrape==0.6.2.20230321.dev13+g786815d) (2023.5.7)\r\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->snscrape==0.6.2.20230321.dev13+g786815d) (1.7.1)\r\n",
      "Building wheels for collected packages: snscrape\r\n",
      "  Building wheel for snscrape (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for snscrape: filename=snscrape-0.6.2.20230321.dev13+g786815d-py3-none-any.whl size=73302 sha256=76e66174970ded506f2bc94f202b7b2cfac64bcfe3c9029e3afc056eca6e307b\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-f9c6qq31/wheels/05/e9/f7/57056e7c7e44b1feed932fa49fdec9d706c4f563e37160ab74\r\n",
      "Successfully built snscrape\r\n",
      "Installing collected packages: snscrape\r\n",
      "Successfully installed snscrape-0.6.2.20230321.dev13+g786815d\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Developer version\n",
    "!pip install git+https://github.com/JustAnotherArchivist/snscrape.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "154ae5f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:08:09.359312Z",
     "iopub.status.busy": "2023-05-27T12:08:09.358920Z",
     "iopub.status.idle": "2023-05-27T12:08:09.680148Z",
     "shell.execute_reply": "2023-05-27T12:08:09.679051Z"
    },
    "papermill": {
     "duration": 0.333742,
     "end_time": "2023-05-27T12:08:09.682885",
     "exception": false,
     "start_time": "2023-05-27T12:08:09.349143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#importing the necessary modules\n",
    "import snscrape.modules.twitter as twitterscraper\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc79d22",
   "metadata": {
    "papermill": {
     "duration": 0.007177,
     "end_time": "2023-05-27T12:08:09.697574",
     "exception": false,
     "start_time": "2023-05-27T12:08:09.690397",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Twitterscraper**\n",
    "\n",
    "There are different methods that can be used to scrape tweets and extract information from Twitter using the snscrape library.\n",
    "\n",
    "TwitterUserScraper: Scrape tweets from a user's timeline.\n",
    "TwitterSearchScraper: Scrape tweets containing specific keywords.\n",
    "TwitterHashtagScraper: Scrape tweets from a specific hashtag.\n",
    "TwitterGeoSearchScraper: Scrape tweets from a specific location based on geolocation.\n",
    "\n",
    "'TwitterHashtagScraper' is used below as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48fe65f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:08:09.714723Z",
     "iopub.status.busy": "2023-05-27T12:08:09.713506Z",
     "iopub.status.idle": "2023-05-27T12:08:09.720543Z",
     "shell.execute_reply": "2023-05-27T12:08:09.719518Z"
    },
    "papermill": {
     "duration": 0.017869,
     "end_time": "2023-05-27T12:08:09.722800",
     "exception": false,
     "start_time": "2023-05-27T12:08:09.704931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scraper=twitterscraper.TwitterHashtagScraper(\"#ViratKohli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd00728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:08:09.741149Z",
     "iopub.status.busy": "2023-05-27T12:08:09.740226Z",
     "iopub.status.idle": "2023-05-27T12:08:09.744615Z",
     "shell.execute_reply": "2023-05-27T12:08:09.743901Z"
    },
    "papermill": {
     "duration": 0.016306,
     "end_time": "2023-05-27T12:08:09.746634",
     "exception": false,
     "start_time": "2023-05-27T12:08:09.730328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets=scraper.get_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34c3b6ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:08:09.763391Z",
     "iopub.status.busy": "2023-05-27T12:08:09.762716Z",
     "iopub.status.idle": "2023-05-27T12:08:09.766555Z",
     "shell.execute_reply": "2023-05-27T12:08:09.765839Z"
    },
    "papermill": {
     "duration": 0.01475,
     "end_time": "2023-05-27T12:08:09.768797",
     "exception": false,
     "start_time": "2023-05-27T12:08:09.754047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweetsf=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34dd2cf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:08:09.785304Z",
     "iopub.status.busy": "2023-05-27T12:08:09.784944Z",
     "iopub.status.idle": "2023-05-27T12:08:47.687007Z",
     "shell.execute_reply": "2023-05-27T12:08:47.685965Z"
    },
    "papermill": {
     "duration": 37.913575,
     "end_time": "2023-05-27T12:08:47.689731",
     "exception": false,
     "start_time": "2023-05-27T12:08:09.776156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/1940205695.py:4: DeprecatedFeatureWarning: content is deprecated, use rawContent instead\n",
      "  tweetsf.append(tweet.content)\n"
     ]
    }
   ],
   "source": [
    "for i,tweet in enumerate(tweets):\n",
    "    if i>1000:\n",
    "        break\n",
    "    tweetsf.append(tweet.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f18661d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:08:47.706860Z",
     "iopub.status.busy": "2023-05-27T12:08:47.706400Z",
     "iopub.status.idle": "2023-05-27T12:08:47.713166Z",
     "shell.execute_reply": "2023-05-27T12:08:47.712191Z"
    },
    "papermill": {
     "duration": 0.018025,
     "end_time": "2023-05-27T12:08:47.715432",
     "exception": false,
     "start_time": "2023-05-27T12:08:47.697407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(tweetsf)):\n",
    "    tweetsf[i]=tweetsf[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aab396f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:08:47.732072Z",
     "iopub.status.busy": "2023-05-27T12:08:47.731640Z",
     "iopub.status.idle": "2023-05-27T12:08:47.743543Z",
     "shell.execute_reply": "2023-05-27T12:08:47.742467Z"
    },
    "papermill": {
     "duration": 0.022717,
     "end_time": "2023-05-27T12:08:47.745720",
     "exception": false,
     "start_time": "2023-05-27T12:08:47.723003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Tweet':tweetsf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ead7d24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:08:47.762848Z",
     "iopub.status.busy": "2023-05-27T12:08:47.762060Z",
     "iopub.status.idle": "2023-05-27T12:08:47.792022Z",
     "shell.execute_reply": "2023-05-27T12:08:47.790641Z"
    },
    "papermill": {
     "duration": 0.041301,
     "end_time": "2023-05-27T12:08:47.794475",
     "exception": false,
     "start_time": "2023-05-27T12:08:47.753174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@cricbuzz virat kohli no 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the tweet you know...\\nthe reason you also kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@bhattiohi @imshubhh16 @cricketracker chal nik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>record ⏺️ alert: 12 centuries 💯 are hitted in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>সম্প্রতি এক অনুষ্ঠানে এসে বেশ মজার মুডে দেখা গ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>यह मनोरंजक कहानी आपको दुनिया के अतीत, वर्तमान ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@pkgshashank vo hai b toh virat kohli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jacqueline fernandez beautiful actress leaked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>virat kohli\\nbhuvneshwar kumar\\nkane williamson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"will have to deliver...\": ex-bcci chief selec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet\n",
       "0                         @cricbuzz virat kohli no 3\n",
       "1  the tweet you know...\\nthe reason you also kno...\n",
       "2  @bhattiohi @imshubhh16 @cricketracker chal nik...\n",
       "3  record ⏺️ alert: 12 centuries 💯 are hitted in ...\n",
       "4  সম্প্রতি এক অনুষ্ঠানে এসে বেশ মজার মুডে দেখা গ...\n",
       "5  यह मनोरंजक कहानी आपको दुनिया के अतीत, वर्तमान ...\n",
       "6              @pkgshashank vo hai b toh virat kohli\n",
       "7  jacqueline fernandez beautiful actress leaked ...\n",
       "8    virat kohli\\nbhuvneshwar kumar\\nkane williamson\n",
       "9  \"will have to deliver...\": ex-bcci chief selec..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cd0e71",
   "metadata": {
    "papermill": {
     "duration": 0.007453,
     "end_time": "2023-05-27T12:08:47.810498",
     "exception": false,
     "start_time": "2023-05-27T12:08:47.803045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "The snscrape library provides support for scraping data from various social media platforms in addition to Twitter. Here are some of the other platforms that you can scrape using snscrape:\n",
    "\n",
    "**Reddit**: You can scrape data from Reddit, including posts and comments, using the *snscrape.modules.reddit* module.\n",
    "\n",
    "**Instagram**: snscrape allows you to scrape data from Instagram, such as posts and comments. However, please note that Instagram's terms of service prohibit automated scraping, so make sure to comply with their policies.\n",
    "\n",
    "**YouTube**: You can scrape data from YouTube, including video metadata and comments, using the *snscrape.modules.youtube* module.\n",
    "\n",
    "**TikTok**: snscrape supports scraping data from TikTok, such as user profiles and videos, using the *snscrape.modules.tiktok* module.\n",
    "\n",
    "**Weibo**: snscrape enables scraping data from Weibo, a popular Chinese social media platform, using the *snscrape.modules.weibo* module.\n",
    "\n",
    "**Pinterest**: snscrape provides support for scraping data from Pinterest, including pins, boards, and user profiles, using the *snscrape.modules.pinterest* module.\n",
    "\n",
    "It's recommended to refer to the snscrape documentation for more details on each specific platform: https://github.com/JustAnotherArchivist/snscrape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 95.607587,
   "end_time": "2023-05-27T12:08:48.740864",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-27T12:07:13.133277",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
